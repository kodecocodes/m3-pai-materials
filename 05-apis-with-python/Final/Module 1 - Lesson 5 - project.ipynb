{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35754509",
   "metadata": {},
   "source": [
    "# Lesson 5 project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d5ac4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### This Demo Requires an OpenAI “Secret Key”\n",
    "\n",
    "Unlike the APIs in the previous demo, OpenAI requires users of its APIs to register for an account and use that account to create one or more API keys — which they call _secret keys_ — in order to use the API. The API is _not_ free of charge to use, which means that you need a credit card, which you use to add money to your OpenAI account balance.\n",
    "\n",
    "Fortunately, it doesn’t cost very much money to use the OpenAI API when you’re using it on personal projects. At the time of writing, US$5.00 should be enough to cover thousands of input and output messages to and from the API.\n",
    "\n",
    "OpenAI, like many other companies that provide generative AI APIs, likes to change the user interface on their dashboards often. Because of this, we have to leave it to you to perform the following steps on your own:\n",
    "\n",
    "1. Sign up for an OpenAI account.\n",
    "2. While logged into the OpenAI dashboard under your account, create a secret key.\n",
    "\n",
    "Once you have created a secret key, copy that key. Create a new text file named `.env` containing the following:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY = replace_this_with_your_secret_key\n",
    "```\n",
    "\n",
    "Replace `replace_this_with_your_secret_key` with your secret key and save the file to the directory where you will create the Jupyter notebook for this demo. By storing the API key in a .env file (and making sure that the .env is _not_ added to source control), you avoid “hard-coding” the API key into your application, which makes it more likely that someone unauthorized will discover it.\n",
    "\n",
    "### Install the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ca17f-d310-490f-930b-554a6dd2e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dotenv\n",
    "# If the above dotenv installation doesn’t work, try\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "# If the above openai installation doesn’t work, try\n",
    "# pip3 install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c70c1",
   "metadata": {},
   "source": [
    "These `pip` commands will install the following libraries:\n",
    "\n",
    "- `dotenv`, which provides functions for reading `.env` files (such as the one you just created) to create environment variables. \n",
    "- `openai`, which provides functionality for communicating with OpenAI’s AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e4347",
   "metadata": {},
   "source": [
    "## Creating an OpenAI Client Object\n",
    "\n",
    "The first step in building an OpenAI-powered application is to create an OpenAI client object. This object enables access to OpenAI’s various APIs, including the two we’ll use: GPT and DALL-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d709a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# When called without arguments, OpenAI will try to get\n",
    "# the API key from the OPENAI_API_KEY environment variable.\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75538c-ea36-4e30-b864-8d3f83e68f97",
   "metadata": {},
   "source": [
    "The `load_dotenv()` method, provided by the `dotenv` library, reads the contents of the `.env` file to create the `OPENAI_API_KEY` environment variable. This variable isn’t a variable in your application, but in your operating system, the environent where your application runs (hence the name).\n",
    "\n",
    "The call to `OpenAI()`, which creates an OpenAI client object, looks for an environment variable named `OPENAI_API_KEY`. If it finds this environment variable, and if its value corresponds to a legitimate API key from an OpenAI account with money it its balance, the client object is instantiated and can be used to access OpenAI’s APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a45ba-4364-45c3-9459-f1d22a7d2545",
   "metadata": {},
   "source": [
    "## Creating Chat Completions\n",
    "\n",
    "When you use ChatGPT, you are providing an OpenAI LLM with prompts and it responds with _chat completions_. To access OpenAI’s LLMs, you use the client object’s `chat` property, whose properties provide access to its chat completion API.\n",
    "\n",
    "### Defining a Chat Completion Function\n",
    "\n",
    "Let’s create a function that generates a “single-turn” completion, where an OpenAI GPT model is given a single prompt and generates a response or solution in a single step, without any follow-up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac423b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_completion(\n",
    "    user_prompt, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    system_prompt=\"\"\"\n",
    "    You are a highly knowledgeable, conversational, and polite assistant capable of\n",
    "    discussing a wide range of topics, from casual conversations to complex technical subjects.\n",
    "    Your goal is to provide accurate, concise, and clear answers while maintaining\n",
    "    a friendly and approachable tone.\n",
    "    \"\"\", \n",
    "    temperature=1.0\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ede70",
   "metadata": {},
   "source": [
    "This function, `create_completion()`, simplifies making a call to the OpenAI client’s `chat.completions.create()` method, which creates chat completions based on a list of messages that you pass to it.\n",
    "\n",
    "In our call to `chat.completions.create()`, we provide arguments for the following parameters:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Parameter</strong></td>\n",
    "        <td style=\"text-align:left;\"><strong>Description</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>model</code></td>\n",
    "        <td>\n",
    "            <p>A string value that specifies which LLM should provide the completion. In this demo, we’ll use `gpt-4o-mini`, the smallest, quickest, and cheapest of the current models, and more than enough for our application.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>temperature</code></td>\n",
    "        <td>\n",
    "            <p>A float value that controls the level of randomness in the completion text that the model generates. Temperatures closer to 0 result in completions that are generated more quickly, and are more deterministic and predictable. At the default value of 1, the completions are supposed to responses with a mix of creativity and consistency. You can provide temperatures higher than 1 for even more creative results (that take longer to generate), but at around 1.4 and higher, the LLM tends to produce nonsensical results.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>messages</code></td>\n",
    "        <td style=\"text-align:left;\">\n",
    "            <p>A list of dictionaries, where each dictionary represents a message to the LLM. Since this demo in concerned only with a single-turn completion, the `messages` list will contain only two message dictionaries:</p>\n",
    "            <ol>\n",
    "                <li>A message with instructions that define how the LLM should behave or responds. This is called a <em>system message</em>, and it’s specified as such by setting the value of the <code>role</code> key to <code>system</code>. The actual message is defined as the value for the <code>content</code> key.\n",
    "                </li>\n",
    "                <li>A message containing the user’s prompt. This is called a _user message_, which is designated by setting the value of the `role` key to `user`. The user’s prompt is used as the value for the <code>content</code> key.</li>\n",
    "            </ol>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "`chat.completions.create()` returns a completion object containing all sorts of information that’s beyond the scope of this demo. The part of the object we’re most interested in is its `choices[0].message.content` property, which contains the completion generated by the LLM. This is what our `create_completion()` function returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716df1f",
   "metadata": {},
   "source": [
    "## Create a chat completion function\n",
    "\n",
    "`chat.completions.create()` returns a completion object containing all sorts of information that’s beyond the scope of this demo. The part of the object we’re most interested in is its `choices[0].message.content` property, which contains the completion generated by the LLM. This is what our `create_completion()` function returns.\n",
    "\n",
    "### Using the Chat Completion Function\n",
    "\n",
    "Test the newly-defined `create_completion()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10accf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_completion(\"I need a character for a pulp novel about a trucker who moonlights as a private detective and gets into wacky but plausible adventures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d219239e",
   "metadata": {},
   "source": [
    "See how creative the LLM can get by boosting the temperature to `1.25`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba402861-d7b9-4fc7-9661-9653f5881404",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_completion(\"I need a character for a pulp novel about a trucker who moonlights as a private detective and gets into wacky but plausible adventures.\", temperature=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b593b14-dac3-446b-b0bc-87a1665433e6",
   "metadata": {},
   "source": [
    "And finally, observe what happens when you _really_ heat things up and bring up the temperature to `1.6`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16232c1-dace-44af-9be7-e77fd763cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_completion(\"I need a character for a pulp novel about a trucker who moonlights as a private detective and gets into wacky but plausible adventures.\", temperature=1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d528d63-7d53-43bd-a6a6-dab541c36e18",
   "metadata": {},
   "source": [
    "The other thing you can experiment with is `create_completion()`’s `system_prompt` parameter. Try overriding the default system message by instructing the LLM to behave very differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282e6d7-fee0-47e1-9e51-233547c3be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_completion(\"Explain the second law of thermodynamics.\", system_prompt=\"You are the stereotypical used car salesperson from a late 1970s low-budget comedy.\", temperature=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e5ac4-7093-41a7-b9ac-acf3562a308b",
   "metadata": {},
   "source": [
    "## Creating AI-Generated Images\n",
    "\n",
    "OpenAI’s text-to-image AI model DALL-E debuted in January 2021. It preceded ChatGPT by nearly two years, and its API release in November 2022 was timed to coincide the launch of ChatGPT (whose API would not be released to the public until march 2023). To access the DALL-E API, you use the client object’s `images` property.\n",
    "\n",
    "### Defining an Image Generation Function\n",
    "\n",
    "Just as we wrote a function to simplify the function call to OpenAI for a chat completion, let’s write a function to simply the OpenAI function call for generating an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a2cd2-04e1-4bfa-bed4-7ef7c6f17435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(\n",
    "    user_prompt,\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\"\n",
    "):\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=user_prompt,\n",
    "        size=size,\n",
    "        quality=quality\n",
    "    )\n",
    "    return response.data[0].url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ba296-0a06-46aa-b31b-d47dec299152",
   "metadata": {},
   "source": [
    "This function, `create_image()`, simplifies making a call to the OpenAI client’s `images.generate()` method, which creates images based on the prompt that you pass to it.\n",
    "\n",
    "In our call to `images.generate()`, we provide arguments for the following parameters:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Parameter</strong></td>\n",
    "        <td style=\"text-align:left;\"><strong>Description</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>model</code></td>\n",
    "        <td>\n",
    "            <p>A string value that specifies which model should generate the completion. In this demo, we’ll use <code>dall-e-3</code>. This is not the default model — that’s <code>dall-e-2</code> — but this one generates more satisfying images.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>prompt</code></td>\n",
    "        <td>\n",
    "            <p>A string value containing a description of the image to be generated.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>size</code></td>\n",
    "        <td>\n",
    "            <p>A string specifying one of a small set of image sizes. We’re using the default <code>1024x1024</code> value in this demo; you can see the full set of sizes and the models for which they are applicable in <a href=\"https://platform.openai.com/docs/api-reference/images/create\">OpenAI’s images API documentation</a>.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>quality</code></td>\n",
    "        <td>\n",
    "            <p>A string determining the quality of the image to be generated. We’re using the default value of <code>standard</code> for this demo, but for even higher quality images, use the value <code>hd</code>. This parameter works only for the <code>dall-e-3</code> model.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f630c-96a9-4780-8ff4-ccba9986642f",
   "metadata": {},
   "source": [
    "### Using the Image Generation Function\n",
    "\n",
    "Test the newly-defined `create_image()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12086e-da93-4fc6-9ffb-110defee87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_image(\"A shiba inu in a cowboy hat in a retro-western-futuristic rodeo riding a Sputnik satellite that's bucking like a bronco.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a853e8-37bf-4898-a4b6-c03a9d1b5bcb",
   "metadata": {},
   "source": [
    "The result will be a URL for the generated image. The URL will be valid for an hour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eead59-2f7a-4196-b1ad-8c808dad7cc7",
   "metadata": {},
   "source": [
    "## Building a “Smart” Weather Application\n",
    "\n",
    "Let’s take what we’ve learned so far in this demo and the previous demo and build an application that:\n",
    "\n",
    "- Takes the name of a location\n",
    "- Gets the current weather at that location\n",
    "- Delivers the weather report for that location in the form of a poem that also tells you if you should wear a sweater or bring an umbrella\n",
    "- And finally, draws a picture of the weather at that location\n",
    "\n",
    "### Converting the Name of a Place to Latitude and Longitude\n",
    "\n",
    "The Open-Meteo API is the natural choice for getting the current weather for a given location. The problem is that it takes latitude and longitude as its location arguments, not place names. We’ll need some way to convert a location name into coordinates.\n",
    "\n",
    "Fortunately, the [GeoPy](https://geopy.readthedocs.io/en/stable/) has what we need: [Nominatim](https://nominatim.org/), a geocoder, which uses [OpenStreetMap](https://www.openstreetmap.org/) data to convert addresses or place names to latitude and longitude.\n",
    "\n",
    "First, you’ll have to install `GeoPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dba3b-59bf-4bcb-9d8f-50f107572a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install geopy\n",
    "# If the above dotenv installation doesn’t work, try\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cc4ea-79b8-4917-938d-32aed0c01a30",
   "metadata": {},
   "source": [
    "Define a function that uses `Nominatim` to convert an address or place name into its latitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc909c-b12a-410b-9a28-7800e5d30e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def location_name_to_latlong(location_name):\n",
    "    geolocator = Nominatim(user_agent=\"Kodeco - Python for AI demo\")\n",
    "    location = geolocator.geocode(location_name)\n",
    "    return (location.latitude, location.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f30c11-0916-4f4a-8011-8042c403c13e",
   "metadata": {},
   "source": [
    "The code creates `geolocator`, a `Nominatim` geolocator object. The `user_agent` parameter should contain the name of the app using the `Nominatim` service. We use `Nominatim`’s `geocode()` method to get the coordinates for the given location and return those coordinates as a tuple.\n",
    "\n",
    "Try it out by using it to get the latitude and longitude of the original Radio Shack store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f82bc5-f7e7-4d9f-9de2-16b58ecee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the coordinates for the original Radio Shack store?\n",
    "location_name_to_latlong(\"46 Brattle St., Cambridge, MA, USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6066f-ba45-4cb8-9173-fab42371a349",
   "metadata": {},
   "source": [
    "It’s not limited to addresses — if a place is listed in OpenStreetMap, our function can convert its name into coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef329b4-44da-493e-9ccf-aa4716f9de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the coordinates for the roadside attraction known\n",
    "# as “Carhenge” — like Stonehenge, but made of cars?\n",
    "location_name_to_latlong(\"Carhenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f18d7f-8a95-466e-b72d-6d1a5daf9dec",
   "metadata": {},
   "source": [
    "### Getting the Current Weather at a Location\n",
    "\n",
    "Let’s write a couple of functions to get the weather for a given location. This will use the `location_name_to_latlong()` function we just defined and the Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92b2a5-c0a2-4e58-8b9f-4a5f9582462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def celsius_to_fahrenheit(degrees_celsius):\n",
    "    return (degrees_celsius * 1.8) + 32\n",
    "\n",
    "def get_current_weather(location_name):\n",
    "    WEATHER_CODE_TABLE = {\n",
    "        0: \"clear sky\",\n",
    "        1: \"mainly clear\",\n",
    "        2: \"partly cloudy\",\n",
    "        3: \"overcast\",\n",
    "        45: \"fog\",\n",
    "        48: \"depositing rime fog\",\n",
    "        51: \"light drizzle\",\n",
    "        53: \"moderate drizzle\",\n",
    "        55: \"dense drizzle\",\n",
    "        56: \"light freezing drizzle\",\n",
    "        57: \"dense freezing drizzle\",\n",
    "        61: \"slight rain\",\n",
    "        63: \"moderate rain\",\n",
    "        65: \"heavy rain\",\n",
    "        66: \"light freezing rain\",\n",
    "        67: \"heavy freezing rain\",\n",
    "        71: \"slight snow\",\n",
    "        73: \"moderate snow\",\n",
    "        75: \"heavy snow\",\n",
    "        77: \"snow grains\",\n",
    "        80: \"light rain showers\",\n",
    "        81: \"moderate rain showers\",\n",
    "        82: \"violent rain showers\",\n",
    "        85: \"slight snow showers\",\n",
    "        86: \"heavy snow showers\",\n",
    "        95: \"thunderstorm\",\n",
    "        96: \"thunderstorm with slight hail\",\n",
    "        99: \"thunderstorm with heavy hail\",\n",
    "    }\n",
    "    \n",
    "    latitude, longitude = location_name_to_latlong(location_name)\n",
    "    url = \"https://api.open-meteo.com/v1/forecast/\"\n",
    "    query_parameters = {\n",
    "        \"latitude\":  latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current\":   \"weathercode,temperature_2m,cloudcover,relativehumidity_2m\"\n",
    "    }\n",
    "    response = requests.get(url, params=query_parameters)\n",
    "    data = response.json()\n",
    "    result = {\n",
    "        \"weather\": WEATHER_CODE_TABLE.get(data[\"current\"][\"weathercode\"], \"unknown\"),\n",
    "        \"cloud_cover\": data[\"current\"][\"cloudcover\"],\n",
    "        \"temperature\": celsius_to_fahrenheit(data[\"current\"][\"temperature_2m\"]),\n",
    "        \"humidity\": data[\"current\"][\"relativehumidity_2m\"],\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1584a",
   "metadata": {},
   "source": [
    "The `get_current_weather()` method takes a location name, uses `location_name_to_latlong()` to convert it into coordinates, and then uses them as two of the parameters for the GET request it sends to Open-Meteo. The final parameter, `current`, contains the weather information we want in the response:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><strong>Parameter string</strong></td>\n",
    "        <td style=\"text-align:left;\"><strong>Description</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>weathercode</code></td>\n",
    "        <td>\n",
    "            <p>An integer representing the current weather at the location. We’ll use the <code>WEATHER_CODE_TABLE</code> dictionary to convert it into words.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>temperature_2m</code></td>\n",
    "        <td>\n",
    "            <p>A float representing the current temperature at the location at an altitude of 2 meters (about 6 feet) above ground level. This value is in degrees Celsius, but we’ve also provide a function to convert Celsius to Fahrenheit.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>cloudcover</code></td>\n",
    "        <td>\n",
    "            <p>The amount of cloud cover at the location, expressed as a percentage between 0 and 100 inclusive.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>relativehumidity_2m</code></td>\n",
    "        <td>\n",
    "            <p>The relative humidity at the requested location, as measured at a height of 2 meters (about 6 feet) above the ground. This value is expressed as a percentage — between 0 and 100 inclusive.</p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The function returns a dictionary containing the values for the weather properties listed above.\n",
    "\n",
    "Test the method by getting the weather at your current location. For me, that’s Tampa, Florida in the United States:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcac16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather(\"Tampa, Florida, USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b24d8-a629-405d-90fd-28fac28d4ce9",
   "metadata": {},
   "source": [
    "### Generating a Weather Poem\n",
    "\n",
    "We now have everything we need to create the two functions for our smart weather application. Let’s create a function to generate the weather poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29548cb9-5ced-4b57-8adb-a83224b4aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_poem(location):\n",
    "    weather = get_current_weather(location)\n",
    "    weather_prompt = (\n",
    "        f\"The current weather in {location} is {weather['weather']}, \" +\n",
    "        f\"with a temperature of {weather['temperature']} degrees F, \" +\n",
    "        f\"{weather['cloud_cover']}% cloud cover, and \" +\n",
    "        f\"{weather['humidity']}% humidity. \" +\n",
    "        \"Create a poem about this location and its current weather. \" +\n",
    "        \"The poem should also suggest if someone would need a sweater \" +\n",
    "        \"or umbrella for the current weather.\"\n",
    "    )\n",
    "    return create_completion(weather_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4cf54-bccb-4689-b8ff-79375a36d346",
   "metadata": {},
   "source": [
    "`create_weather_poem()` uses the results from `get_current_weather()` to assemble a prompt, which it feeds to `create_completion()` to generate a weather poem.\n",
    "\n",
    "Try it out by generating a weather poem for where you are right now — or any other location where you want to know the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de41b1-44a0-47f0-be7e-c2dc7d885009",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_poem(\"Tampa, Florida, USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1072a-29ba-4508-b5a5-c9378d40bef2",
   "metadata": {},
   "source": [
    "### Generating a Weather Image\n",
    "\n",
    "Generating a weather image follows a process similar to that for generating the weather poem: retrieve the weather information for the given location, use that information to build a prompt, and pass that prompt to the appropriate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88526b7-ffde-4d7e-bf05-572f99d9415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_image(location):\n",
    "    weather = get_current_weather(location)\n",
    "    weather_prompt = (\n",
    "        f\"A skyline view of {location}, where the weather is {weather['weather']}, \" +\n",
    "        f\"with a temperature of {weather['temperature']} degrees F, \" +\n",
    "        f\"{weather['cloud_cover']}% cloud cover, and \" +\n",
    "        f\"{weather['humidity']}% humidity. \" +\n",
    "        \"Include people dressed appropriately for the weather.\"\n",
    "    )\n",
    "    weather_image_url = create_image(weather_prompt)\n",
    "    return weather_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae259262-58a5-45ad-8c6d-2a6e1e0b99a7",
   "metadata": {},
   "source": [
    "Once again, test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c7e1c-9a88-4a5a-92b2-d103771092c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weather_image(\"Tampa, Florida, USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b266578",
   "metadata": {},
   "source": [
    "### Bringing It All Together\n",
    "\n",
    "We’ve done a lot in this demo, so let’s make the application a simple one that asks the user for location and prints out the poem and picture URL. You may find yourself doing it over and over, just to see the what comes out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85bc83-c5cc-413a-bbec-3bec8bc6f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sweater Weather, the poetic weather app\")\n",
    "print(\"=======================================\")\n",
    "location = input(\"What place would you like a weather poem and picture for?\")\n",
    "weather_poem = create_weather_poem(location)\n",
    "weather_image_url = create_weather_image(location)\n",
    "\n",
    "print(f\"A weather poem about {location}:\")\n",
    "print(f\"{weather_poem}\\n\")\n",
    "print(\"And here's a picture (click to view it):\")\n",
    "print(weather_image_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
